{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c977deb-b17a-4d11-a794-15e3305bb86b",
   "metadata": {},
   "source": [
    "## Part one: centroid but not generating centroids , points attached on edge  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee51592-a234-4e8e-b52c-4667f7f485c5",
   "metadata": {},
   "source": [
    "# Creating_Training samples of each year from 2012-2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b7212606-3a20-4d4d-868c-f41cdadc5df2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clipping completed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import rasterio\n",
    "import geopandas as gpd\n",
    "from rasterio.mask import mask\n",
    "\n",
    "# Define the paths to your files\n",
    "raster_path = \"D:/Project_thesis/Data/Reclassified_new/Reclassified_2021.tif\"\n",
    "shapefiles = {\n",
    "    'training': './Study_area/training_set.shp',\n",
    "    'testing': './Study_area/testing_set.shp',\n",
    "    'validation': './Study_area/validation_set.shp'\n",
    "}\n",
    "output_dir = \"D:/Project_thesis/Data/Ground_truth\"\n",
    "\n",
    "# Ensure the output directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Function to clip raster with shapefile\n",
    "def clip_raster(raster_path, shapefile_path, output_path):\n",
    "    with rasterio.open(raster_path) as src:\n",
    "        shapefile = gpd.read_file(shapefile_path)\n",
    "        shapes = [feature[\"geometry\"] for feature in shapefile.__geo_interface__[\"features\"]]\n",
    "        out_image, out_transform = mask(src, shapes, crop=True)\n",
    "        out_meta = src.meta.copy()\n",
    "        out_meta.update({\"driver\": \"GTiff\",\n",
    "                         \"height\": out_image.shape[1],\n",
    "                         \"width\": out_image.shape[2],\n",
    "                         \"transform\": out_transform})\n",
    "\n",
    "        with rasterio.open(output_path, \"w\", **out_meta) as dest:\n",
    "            dest.write(out_image)\n",
    "\n",
    "# Clip raster with each shapefile\n",
    "for key, shapefile_path in shapefiles.items():\n",
    "    output_path = os.path.join(output_dir, f\"Reclassified_2021_{key}.tif\")\n",
    "    clip_raster(raster_path, shapefile_path, output_path)\n",
    "\n",
    "print(\"Clipping completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8baa3653-05b6-4232-b185-f55ebd3b9470",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing and centroid computation completed.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import rasterio\n",
    "\n",
    "# Paths to the input files\n",
    "files = {\n",
    "    'training': 'D:/Project_thesis/Data/Ground_truth/Reclassified_2021_training.tif',\n",
    "    'testing': 'D:/Project_thesis/Data/Ground_truth/Reclassified_2021_testing.tif',\n",
    "    'validation': 'D:/Project_thesis/Data/Ground_truth/Reclassified_2021_validation.tif'\n",
    "}\n",
    "\n",
    "output_dir = \"D:/Project_thesis/Data/Ground_truth/Output/\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Function to read TIFF into numpy array using Rasterio\n",
    "def read_tiff_to_numpy_rasterio(tiff_path):\n",
    "    with rasterio.open(tiff_path) as dataset:\n",
    "        array = dataset.read(1)  # Assuming the class labels are in the first band\n",
    "        transform = dataset.transform\n",
    "    return array, transform\n",
    "\n",
    "# Function to sample pixels\n",
    "def sample_pixels(array, num_samples=1000):\n",
    "    sampled_pixels = {}\n",
    "    \n",
    "    for label in range(1, 6):  # Classes 1 to 5\n",
    "        indices = np.column_stack(np.where(array == label))\n",
    "        if len(indices) < num_samples:\n",
    "            raise ValueError(f\"Not enough pixels for class {label} in the provided raster.\")\n",
    "        \n",
    "        sampled_indices = indices[np.random.choice(indices.shape[0], num_samples, replace=False)]\n",
    "        sampled_pixels[label] = sampled_indices\n",
    "    \n",
    "    return sampled_pixels\n",
    "\n",
    "# Function to compute centroids from sampled pixels\n",
    "def compute_centroids(sampled_pixels, transform):\n",
    "    centroids = []\n",
    "    for class_value, pixels in sampled_pixels.items():\n",
    "        points = []\n",
    "        for pixel in pixels:\n",
    "            x, y = rasterio.transform.xy(transform, pixel[0], pixel[1])\n",
    "            points.append((x, y))\n",
    "        points_array = np.array(points)\n",
    "        centroid_x = np.mean(points_array[:, 0])\n",
    "        centroid_y = np.mean(points_array[:, 1])\n",
    "        centroids.append({'class': class_value, 'geometry': Point(centroid_x, centroid_y)})\n",
    "    return centroids\n",
    "\n",
    "# Process each file\n",
    "for key, file_path in files.items():\n",
    "    array, transform = read_tiff_to_numpy_rasterio(file_path)\n",
    "    sampled_pixels = sample_pixels(array)\n",
    "    \n",
    "    centroids = compute_centroids(sampled_pixels, transform)\n",
    "    \n",
    "    # Save centroids to shapefile\n",
    "    gdf = gpd.GeoDataFrame(centroids, crs=\"EPSG:4326\")  # Change CRS as needed\n",
    "    output_shapefile = os.path.join(output_dir, f\"{key}_centroids.shp\")\n",
    "    gdf.to_file(output_shapefile, driver=\"ESRI Shapefile\")\n",
    "\n",
    "print(\"Processing and centroid computation completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc174242-1924-4d30-980c-3a9db967ee1c",
   "metadata": {},
   "source": [
    "## Step 1: Read the TIFF files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "93353dbf-eae8-4bb7-91bf-18a8a315fa20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import glob\n",
    "from osgeo import gdal, ogr, osr\n",
    "\n",
    "def read_tiff_to_numpy_gdal(tiff_path):\n",
    "    dataset = gdal.Open(tiff_path)\n",
    "    band = dataset.GetRasterBand(1)  # Assuming class labels are in the first band\n",
    "    array = band.ReadAsArray()\n",
    "    return array, dataset.GetGeoTransform()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9315b5b3-0183-4a31-9463-602c8d8ecfb9",
   "metadata": {},
   "source": [
    "## Step 2: Sample 1000 pixels per class (1 to 5) for each file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a9f83c4e-e145-4fd9-8d7c-727c9b292699",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sample_pixels(array, num_samples=1000):\n",
    "    unique_classes = np.unique(array)\n",
    "    sampled_pixels = {label: None for label in unique_classes if label in [1, 2, 3, 4, 5]}\n",
    "    \n",
    "    for label in sampled_pixels.keys():\n",
    "        indices = np.where(array == label)\n",
    "        sampled_indices = np.random.choice(range(len(indices[0])), num_samples, replace=False)\n",
    "        sampled_pixels[label] = (indices[0][sampled_indices], indices[1][sampled_indices])\n",
    "    \n",
    "    return sampled_pixels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828890fb-17c4-4109-9a75-94971a137df3",
   "metadata": {},
   "source": [
    "### Step 3: Convert pixel indices to geographic coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "68a6e25f-7de6-4967-aeb2-da763aade700",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pixel_to_coord(x, y, geotransform):\n",
    "    x_geo = geotransform[0] + geotransform[1] * x + geotransform[2] * y\n",
    "    y_geo = geotransform[3] + geotransform[4] * x + geotransform[5] * y\n",
    "    return (x_geo, y_geo)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8dbc6ed-c56e-4b1e-8026-970967e2a90b",
   "metadata": {},
   "source": [
    "### Step 4: Create shapefile from selected pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "98f9ca8d-ea05-4cb3-bb02-ff48c30b7fd2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_shapefile_from_selected_pixels(selected_pixels, shapefile_path, geotransform):\n",
    "    driver = ogr.GetDriverByName('ESRI Shapefile')\n",
    "    if os.path.exists(shapefile_path):\n",
    "        driver.DeleteDataSource(shapefile_path)\n",
    "    data_source = driver.CreateDataSource(shapefile_path)\n",
    "    srs = osr.SpatialReference()\n",
    "    srs.ImportFromEPSG(4326)  # WGS84\n",
    "    \n",
    "    layer = data_source.CreateLayer(shapefile_path, srs, ogr.wkbPoint)\n",
    "    layer.CreateField(ogr.FieldDefn(\"ID\", ogr.OFTInteger))\n",
    "    layer.CreateField(ogr.FieldDefn(\"ClassID\", ogr.OFTInteger))\n",
    "    layer.CreateField(ogr.FieldDefn(\"ClassName\", ogr.OFTString))\n",
    "    \n",
    "    class_names = {1: \"Antropic\", 2: \"Primary forest\", 3: \"Secondary forest\", 4: \"Deforestation\", 5: \"Water\"}\n",
    "    \n",
    "    point_id = 1  # Initialize point ID counter\n",
    "    \n",
    "    for class_label, (y_indices, x_indices) in selected_pixels.items():\n",
    "        class_name = class_names.get(class_label, \"Unknown\")\n",
    "        for y, x in zip(y_indices, x_indices):\n",
    "            x_geo, y_geo = pixel_to_coord(x, y, geotransform)\n",
    "            \n",
    "            feature = ogr.Feature(layer.GetLayerDefn())\n",
    "            feature.SetField(\"ID\", point_id)  # Set the point ID\n",
    "            feature.SetField(\"ClassID\", int(class_label))\n",
    "            feature.SetField(\"ClassName\", class_name)\n",
    "            point = ogr.Geometry(ogr.wkbPoint)\n",
    "            point.AddPoint(x_geo, y_geo)\n",
    "            feature.SetGeometry(point)\n",
    "            layer.CreateFeature(feature)\n",
    "            \n",
    "            point_id += 1  # Increment point ID for the next feature\n",
    "            \n",
    "            feature = None  # Clean up\n",
    "    \n",
    "    data_source = None\n",
    "    print(f\"Shapefile created: {shapefile_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "348bdc51-8f40-4eb1-b7b1-18300d3f7fbe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapefile created: D:/Project_thesis/Data/Ground_truth/Output/training\\training_2021.shp\n",
      "Shapefile created: D:/Project_thesis/Data/Ground_truth/Output/testing\\testing_2021.shp\n",
      "Shapefile created: D:/Project_thesis/Data/Ground_truth/Output/validation\\validation_2021.shp\n"
     ]
    }
   ],
   "source": [
    "# New structure for saving shapefiles\n",
    "base_shapefiles_path = \"D:/Project_thesis/Data/Ground_truth/Output/\"\n",
    "\n",
    "if not os.path.exists(base_shapefiles_path):\n",
    "    os.makedirs(base_shapefiles_path)\n",
    "\n",
    "image_types = ['training', 'testing', 'validation']\n",
    "\n",
    "for image_type in image_types:\n",
    "    # Determine the appropriate subdirectory for shapefiles based on image type\n",
    "    shapefiles_sub_path = os.path.join(base_shapefiles_path, image_type)\n",
    "    if not os.path.exists(shapefiles_sub_path):\n",
    "        os.makedirs(shapefiles_sub_path)\n",
    "    \n",
    "    pattern = f'D:/Project_thesis/Data/Ground_truth/Reclassified_2021_{image_type}.tif'\n",
    "    \n",
    "    array, geotransform = read_tiff_to_numpy_gdal(pattern)\n",
    "    sampled_pixels = sample_pixels(array)\n",
    "    # Naming: e.g., \"training_2021.shp\"\n",
    "    shapefile_name = f\"{image_type}_2021.shp\"\n",
    "    shapefile_path = os.path.join(shapefiles_sub_path, shapefile_name)\n",
    "    create_shapefile_from_selected_pixels(sampled_pixels, shapefile_path, geotransform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d83d82-98e6-47ff-a499-616633ef461d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "563dfd59-dd8e-4005-a9a3-f5a34935921e",
   "metadata": {},
   "source": [
    "## part two code "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b166f8-8708-4275-8543-945e8b4d1447",
   "metadata": {},
   "source": [
    "## Step 1: Read the TIFF files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "00b5e253-4537-4b90-80ef-ddb39b46b957",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from osgeo import gdal, ogr, osr\n",
    "from shapely.geometry import Point\n",
    "from sklearn.cluster import KMeans\n",
    "import warnings\n",
    "\n",
    "def read_tiff_to_numpy_gdal(tiff_path):\n",
    "    dataset = gdal.Open(tiff_path)\n",
    "    band = dataset.GetRasterBand(1)  # Assuming class labels are in the first band\n",
    "    array = band.ReadAsArray()\n",
    "    return array, dataset.GetGeoTransform()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045cebb4-0d00-4a10-899d-5415fa1013fd",
   "metadata": {},
   "source": [
    "## Step 2: Sample 1000 pixels per class (1 to 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "31e4351d-a1a0-4cef-95f8-eccaeb2ce57b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sample_pixels(array, num_samples=100):\n",
    "    unique_classes = np.unique(array)\n",
    "    sampled_pixels = {label: None for label in unique_classes if label in [1, 2, 3, 4, 5]}\n",
    "    \n",
    "    for label in sampled_pixels.keys():\n",
    "        indices = np.where(array == label)\n",
    "        if len(indices[0]) < num_samples:\n",
    "            raise ValueError(f\"Not enough pixels for class {label} in the provided raster.\")\n",
    "        sampled_indices = np.random.choice(range(len(indices[0])), num_samples, replace=False)\n",
    "        sampled_pixels[label] = (indices[0][sampled_indices], indices[1][sampled_indices])\n",
    "    \n",
    "    return sampled_pixels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e24827d-9cc1-494e-8b8c-4f3540cac252",
   "metadata": {},
   "source": [
    "## Step 3: Convert pixel indices to geographic coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "997b2481-7c30-4df1-af5e-180279f95874",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pixel_to_coord(x, y, geotransform):\n",
    "    x_geo = geotransform[0] + geotransform[1] * (x + 0.5) + geotransform[2] * (y + 0.5)\n",
    "    y_geo = geotransform[3] + geotransform[4] * (x + 0.5) + geotransform[5] * (y + 0.5)\n",
    "    return (x_geo, y_geo)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4091f3-b9b5-463e-bcc7-e449995c93e2",
   "metadata": {},
   "source": [
    "## Step 4: Cluster sampled pixels and compute centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a3952670-fe0a-4c02-aab5-75065043dd48",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def cluster_pixels_and_compute_centroids(sampled_pixels, geotransform, num_clusters=1000):\n",
    "    centroids = []\n",
    "    point_id = 1\n",
    "    class_names = {1: \"Antropic\", 2: \"Primary forest\", 3: \"Secondary forest\", 4: \"Deforestation\", 5: \"Water\"}\n",
    "    \n",
    "    for class_value, (y_indices, x_indices) in sampled_pixels.items():\n",
    "        points = [pixel_to_coord(x, y, geotransform) for y, x in zip(y_indices, x_indices)]\n",
    "        points_array = np.array(points)\n",
    "        \n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "            kmeans = KMeans(n_clusters=num_clusters, n_init='auto')\n",
    "            kmeans.fit(points_array)\n",
    "        \n",
    "        cluster_centers = kmeans.cluster_centers_\n",
    "        \n",
    "        for center in cluster_centers:\n",
    "            centroids.append({\n",
    "                'ID': point_id,\n",
    "                'class': int(class_value),\n",
    "                'ClassName': class_names[class_value],\n",
    "                'geometry': Point(center[0], center[1])\n",
    "            })\n",
    "            point_id += 1\n",
    "    \n",
    "    return centroids\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d8e1af-cd24-4347-ba0f-66a7c97e07a7",
   "metadata": {},
   "source": [
    "## Step 5: Create shapefile from centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e2e289a5-4872-4fa3-a99e-fab585e8e829",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_shapefile_from_centroids(centroids, shapefile_path):\n",
    "    driver = ogr.GetDriverByName('ESRI Shapefile')\n",
    "    if driver is None:\n",
    "        raise RuntimeError(\"ESRI Shapefile driver is not available.\")\n",
    "    \n",
    "    if os.path.exists(shapefile_path):\n",
    "        driver.DeleteDataSource(shapefile_path)\n",
    "    \n",
    "    data_source = driver.CreateDataSource(shapefile_path)\n",
    "    if data_source is None:\n",
    "        raise RuntimeError(f\"Failed to create data source: {shapefile_path}\")\n",
    "    \n",
    "    srs = osr.SpatialReference()\n",
    "    srs.ImportFromEPSG(4326)  # WGS84\n",
    "    \n",
    "    layer = data_source.CreateLayer('', srs, ogr.wkbPoint)\n",
    "    if layer is None:\n",
    "        raise RuntimeError(f\"Failed to create layer in shapefile: {shapefile_path}\")\n",
    "    \n",
    "    layer.CreateField(ogr.FieldDefn(\"ID\", ogr.OFTInteger))\n",
    "    layer.CreateField(ogr.FieldDefn(\"ClassID\", ogr.OFTInteger))\n",
    "    layer.CreateField(ogr.FieldDefn(\"ClassName\", ogr.OFTString))\n",
    "    \n",
    "    for centroid in centroids:\n",
    "        feature = ogr.Feature(layer.GetLayerDefn())\n",
    "        feature.SetField(\"ID\", centroid['ID'])  # Set the point ID\n",
    "        feature.SetField(\"ClassID\", int(centroid['class']))\n",
    "        feature.SetField(\"ClassName\", centroid['ClassName'])\n",
    "        \n",
    "        geometry = centroid['geometry']\n",
    "        point = ogr.Geometry(ogr.wkbPoint)\n",
    "        point.AddPoint(geometry.x, geometry.y)\n",
    "        feature.SetGeometry(point)\n",
    "        layer.CreateFeature(feature)\n",
    "        \n",
    "        feature = None  # Clean up\n",
    "    \n",
    "    data_source = None\n",
    "    print(f\"Shapefile created: {shapefile_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1698a4-d86b-44c2-9855-6078fb6d069c",
   "metadata": {},
   "source": [
    "## Step 6: Process each TIFF file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "6d32c98e-98a8-41eb-9363-03431d912b4c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n",
      "  warnings.warn(\n",
      "C:\\Users\\User\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n",
      "  warnings.warn(\n",
      "C:\\Users\\User\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n",
      "  warnings.warn(\n",
      "C:\\Users\\User\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n",
      "  warnings.warn(\n",
      "C:\\Users\\User\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapefile created: D:/Project_thesis/Data/Ground_truth/Output500/training_centroids.shp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n",
      "  warnings.warn(\n",
      "C:\\Users\\User\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n",
      "  warnings.warn(\n",
      "C:\\Users\\User\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n",
      "  warnings.warn(\n",
      "C:\\Users\\User\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n",
      "  warnings.warn(\n",
      "C:\\Users\\User\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapefile created: D:/Project_thesis/Data/Ground_truth/Output500/testing_centroids.shp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n",
      "  warnings.warn(\n",
      "C:\\Users\\User\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n",
      "  warnings.warn(\n",
      "C:\\Users\\User\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n",
      "  warnings.warn(\n",
      "C:\\Users\\User\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n",
      "  warnings.warn(\n",
      "C:\\Users\\User\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapefile created: D:/Project_thesis/Data/Ground_truth/Output500/validation_centroids.shp\n",
      "Processing and centroid computation completed.\n"
     ]
    }
   ],
   "source": [
    "# Paths to the input files\n",
    "files = {\n",
    "    'training': 'D:/Project_thesis/Data/Ground_truth/Reclassified_2021_training.tif',\n",
    "    'testing': 'D:/Project_thesis/Data/Ground_truth/Reclassified_2021_testing.tif',\n",
    "    'validation': 'D:/Project_thesis/Data/Ground_truth/Reclassified_2021_validation.tif'\n",
    "}\n",
    "\n",
    "output_dir = \"D:/Project_thesis/Data/Ground_truth/Output500/\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for key, file_path in files.items():\n",
    "    array, geotransform = read_tiff_to_numpy_gdal(file_path)\n",
    "    sampled_pixels = sample_pixels(array)\n",
    "    \n",
    "    centroids = cluster_pixels_and_compute_centroids(sampled_pixels, geotransform)\n",
    "    \n",
    "    # Save centroids to shapefile\n",
    "    shapefile_name = f\"{key}_centroids.shp\"\n",
    "    shapefile_path = os.path.join(output_dir, shapefile_name)\n",
    "    create_shapefile_from_centroids(centroids, shapefile_path)\n",
    "\n",
    "print(\"Processing and centroid computation completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a7e595-6c64-4788-b73d-9cdb75139e4f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_shapefile_from_centroids(centroids, shapefile_path):\n",
    "    driver = ogr.GetDriverByName('ESRI Shapefile')\n",
    "    if driver is None:\n",
    "        raise RuntimeError(\"ESRI Shapefile driver is not available.\")\n",
    "    \n",
    "    if os.path.exists(shapefile_path):\n",
    "        driver.DeleteDataSource(shapefile_path)\n",
    "    \n",
    "    data_source = driver.CreateDataSource(shapefile_path)\n",
    "    if data_source is None:\n",
    "        raise RuntimeError(f\"Failed to create data source: {shapefile_path}\")\n",
    "    \n",
    "    srs = osr.SpatialReference()\n",
    "    srs.ImportFromEPSG(4326)  # WGS84\n",
    "    \n",
    "    layer = data_source.CreateLayer('', srs, ogr.wkbPoint)\n",
    "    if layer is None:\n",
    "        raise RuntimeError(f\"Failed to create layer in shapefile: {shapefile_path}\")\n",
    "    \n",
    "    layer.CreateField(ogr.FieldDefn(\"ID\", ogr.OFTInteger))\n",
    "    layer.CreateField(ogr.FieldDefn(\"ClassID\", ogr.OFTInteger))\n",
    "    layer.CreateField(ogr.FieldDefn(\"ClassName\", ogr.OFTString))\n",
    "    \n",
    "    for centroid in centroids:\n",
    "        feature = ogr.Feature(layer.GetLayerDefn())\n",
    "        feature.SetField(\"ID\", centroid['ID'])  # Set the point ID\n",
    "        feature.SetField(\"ClassID\", int(centroid['class']))\n",
    "        feature.SetField(\"ClassName\", centroid['ClassName'])\n",
    "        \n",
    "        geometry = centroid['geometry']\n",
    "        point = ogr.Geometry(ogr.wkbPoint)\n",
    "        point.AddPoint(geometry.x, geometry.y)\n",
    "        feature.SetGeometry(point)\n",
    "        layer.CreateFeature(feature)\n",
    "        \n",
    "        feature = None  # Clean up\n",
    "    \n",
    "    data_source = None\n",
    "    print(f\"Shapefile created: {shapefile_path}\")\n",
    "\n",
    "# Paths to the input files\n",
    "files = {\n",
    "    'training': 'D:/Project_thesis/Data/Ground_truth/Reclassified_2021_training.tif',\n",
    "    'testing': 'D:/Project_thesis/Data/Ground_truth/Reclassified_2021_testing.tif',\n",
    "    'validation': 'D:/Project_thesis/Data/Ground_truth/Reclassified_2021_validation.tif'\n",
    "}\n",
    "\n",
    "output_dir = \"D:/Project_thesis/Data/Ground_truth/Output/\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Create subdirectories if they don't exist\n",
    "for subdir in files.keys():\n",
    "    os.makedirs(os.path.join(output_dir, subdir), exist_ok=True)\n",
    "\n",
    "for key, file_path in files.items():\n",
    "    array, geotransform = read_tiff_to_numpy_gdal(file_path)\n",
    "    sampled_pixels = sample_pixels(array)\n",
    "    \n",
    "    centroids = cluster_pixels_and_compute_centroids(sampled_pixels, geotransform)\n",
    "    \n",
    "    # Save centroids to shapefile\n",
    "    shapefile_name = f\"{key}_centroids.shp\"\n",
    "    shapefile_path = os.path.join(output_dir, key, shapefile_name)\n",
    "    create_shapefile_from_centroids(centroids, shapefile_path)\n",
    "\n",
    "print(\"Processing and centroid computation completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "94e09fcc-dafd-4438-90de-a9de8d74e35c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n",
      "  warnings.warn(\n",
      "C:\\Users\\User\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n",
      "  warnings.warn(\n",
      "C:\\Users\\User\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n",
      "  warnings.warn(\n",
      "C:\\Users\\User\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n",
      "  warnings.warn(\n",
      "C:\\Users\\User\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapefile created: D:/Project_thesis/Data/Ground_truth/Output500/training\\training_centroids.shp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n",
      "  warnings.warn(\n",
      "C:\\Users\\User\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n",
      "  warnings.warn(\n",
      "C:\\Users\\User\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n",
      "  warnings.warn(\n",
      "C:\\Users\\User\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n",
      "  warnings.warn(\n",
      "C:\\Users\\User\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapefile created: D:/Project_thesis/Data/Ground_truth/Output500/testing\\testing_centroids.shp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n",
      "  warnings.warn(\n",
      "C:\\Users\\User\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n",
      "  warnings.warn(\n",
      "C:\\Users\\User\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n",
      "  warnings.warn(\n",
      "C:\\Users\\User\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapefile created: D:/Project_thesis/Data/Ground_truth/Output500/validation\\validation_centroids.shp\n",
      "Processing and centroid computation completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from osgeo import gdal, ogr, osr\n",
    "from shapely.geometry import Point\n",
    "from sklearn.cluster import KMeans\n",
    "import warnings\n",
    "\n",
    "def read_tiff_to_numpy_gdal(tiff_path):\n",
    "    dataset = gdal.Open(tiff_path)\n",
    "    band = dataset.GetRasterBand(1)  # Assuming class labels are in the first band\n",
    "    array = band.ReadAsArray()\n",
    "    return array, dataset.GetGeoTransform()\n",
    "\n",
    "def sample_pixels(array, num_samples=500):\n",
    "    unique_classes = np.unique(array)\n",
    "    sampled_pixels = {label: None for label in unique_classes if label in [1, 2, 3, 4, 5]}\n",
    "    \n",
    "    for label in sampled_pixels.keys():\n",
    "        indices = np.where(array == label)\n",
    "        if len(indices[0]) < num_samples:\n",
    "            raise ValueError(f\"Not enough pixels for class {label} in the provided raster.\")\n",
    "        sampled_indices = np.random.choice(range(len(indices[0])), num_samples, replace=False)\n",
    "        sampled_pixels[label] = (indices[0][sampled_indices], indices[1][sampled_indices])\n",
    "    \n",
    "    return sampled_pixels\n",
    "\n",
    "def pixel_to_coord(x, y, geotransform):\n",
    "    x_geo = geotransform[0] + geotransform[1] * (x + 0.5) + geotransform[2] * (y + 0.5)\n",
    "    y_geo = geotransform[3] + geotransform[4] * (x + 0.5) + geotransform[5] * (y + 0.5)\n",
    "    return (x_geo, y_geo)\n",
    "\n",
    "def cluster_pixels_and_compute_centroids(sampled_pixels, geotransform, num_clusters=500):\n",
    "    centroids = []\n",
    "    point_id = 1\n",
    "    class_names = {1: \"Antropic\", 2: \"Primary forest\", 3: \"Secondary forest\", 4: \"Deforestation\", 5: \"Water\"}\n",
    "    \n",
    "    for class_value, (y_indices, x_indices) in sampled_pixels.items():\n",
    "        points = [pixel_to_coord(x, y, geotransform) for y, x in zip(y_indices, x_indices)]\n",
    "        points_array = np.array(points)\n",
    "        \n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "            kmeans = KMeans(n_clusters=num_clusters, n_init='auto')\n",
    "            kmeans.fit(points_array)\n",
    "        \n",
    "        cluster_centers = kmeans.cluster_centers_\n",
    "        \n",
    "        for center in cluster_centers:\n",
    "            centroids.append({\n",
    "                'ID': point_id,\n",
    "                'class': int(class_value),\n",
    "                'ClassName': class_names[class_value],\n",
    "                'geometry': Point(center[0], center[1])\n",
    "            })\n",
    "            point_id += 1\n",
    "    \n",
    "    return centroids\n",
    "\n",
    "def create_shapefile_from_centroids(centroids, shapefile_path):\n",
    "    driver = ogr.GetDriverByName('ESRI Shapefile')\n",
    "    if driver is None:\n",
    "        raise RuntimeError(\"ESRI Shapefile driver is not available.\")\n",
    "    \n",
    "    if os.path.exists(shapefile_path):\n",
    "        driver.DeleteDataSource(shapefile_path)\n",
    "    \n",
    "    data_source = driver.CreateDataSource(shapefile_path)\n",
    "    if data_source is None:\n",
    "        raise RuntimeError(f\"Failed to create data source: {shapefile_path}\")\n",
    "    \n",
    "    srs = osr.SpatialReference()\n",
    "    srs.ImportFromEPSG(4326)  # WGS84\n",
    "    \n",
    "    layer = data_source.CreateLayer('', srs, ogr.wkbPoint)\n",
    "    if layer is None:\n",
    "        raise RuntimeError(f\"Failed to create layer in shapefile: {shapefile_path}\")\n",
    "    \n",
    "    layer.CreateField(ogr.FieldDefn(\"ID\", ogr.OFTInteger))\n",
    "    layer.CreateField(ogr.FieldDefn(\"ClassID\", ogr.OFTInteger))\n",
    "    layer.CreateField(ogr.FieldDefn(\"ClassName\", ogr.OFTString))\n",
    "    \n",
    "    for centroid in centroids:\n",
    "        feature = ogr.Feature(layer.GetLayerDefn())\n",
    "        feature.SetField(\"ID\", centroid['ID'])  # Set the point ID\n",
    "        feature.SetField(\"ClassID\", int(centroid['class']))\n",
    "        feature.SetField(\"ClassName\", centroid['ClassName'])\n",
    "        \n",
    "        geometry = centroid['geometry']\n",
    "        point = ogr.Geometry(ogr.wkbPoint)\n",
    "        point.AddPoint(geometry.x, geometry.y)\n",
    "        feature.SetGeometry(point)\n",
    "        layer.CreateFeature(feature)\n",
    "        \n",
    "        feature = None  # Clean up\n",
    "    \n",
    "    data_source = None\n",
    "    print(f\"Shapefile created: {shapefile_path}\")\n",
    "\n",
    "# Paths to the input files\n",
    "files = {\n",
    "    'training': 'D:/Project_thesis/Data/Ground_truth/Reclassified_2021_training.tif',\n",
    "    'testing': 'D:/Project_thesis/Data/Ground_truth/Reclassified_2021_testing.tif',\n",
    "    'validation': 'D:/Project_thesis/Data/Ground_truth/Reclassified_2021_validation.tif'\n",
    "}\n",
    "\n",
    "output_dir = \"D:/Project_thesis/Data/Ground_truth/Output500/\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Create subdirectories if they don't exist\n",
    "for subdir in files.keys():\n",
    "    os.makedirs(os.path.join(output_dir, subdir), exist_ok=True)\n",
    "\n",
    "for key, file_path in files.items():\n",
    "    array, geotransform = read_tiff_to_numpy_gdal(file_path)\n",
    "    sampled_pixels = sample_pixels(array)\n",
    "    \n",
    "    centroids = cluster_pixels_and_compute_centroids(sampled_pixels, geotransform)\n",
    "    \n",
    "    # Save centroids to shapefile\n",
    "    shapefile_name = f\"{key}_centroids.shp\"\n",
    "    shapefile_path = os.path.join(output_dir, key, shapefile_name)\n",
    "    create_shapefile_from_centroids(centroids, shapefile_path)\n",
    "\n",
    "print(\"Processing and centroid computation completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73085d06-41d3-4f9e-92c1-ac8c737c640f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
